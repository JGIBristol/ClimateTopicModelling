{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero-shot Topic modelling: https://maartengr.github.io/BERTopic/getting_started/zeroshot/zeroshot.html#example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note there's a bug/feature of the below function that's sort of the nature of natural language.\n",
    "If you split by \".\" you might occasionally split at an important point: \n",
    "\"I stocked up on supplies e.g. baked beans\" would get split into \"I stocked up on supplies e\", \"g\" and \"baked beans\". \n",
    "\n",
    "Use this approach with caution and check the outputs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_update_indices(test_list, test_index, split_list):\n",
    "    \"\"\"\n",
    "    Splits elements in a list based on a list of splitters and updates the index list accordingly.\n",
    "    Useful below for keeping track of who said what from topic modelling whilst splitting longer responses.\n",
    "\n",
    "    Parameters\n",
    "    test_list : list\n",
    "        List of strings to split.\n",
    "    test_index : list\n",
    "        List of indices corresponding to the strings in test_list.\n",
    "    split_list : list\n",
    "        List of strings to split on.\n",
    "\n",
    "    Returns\n",
    "    new_list : list\n",
    "        List of strings split on the splitters.\n",
    "    new_index : list\n",
    "        List of indices corresponding to the strings in new_list.\n",
    "    \"\"\"\n",
    "    new_list = []\n",
    "    new_index = []\n",
    "\n",
    "    for element, index in zip(test_list, test_index):\n",
    "        split_elements = [element]\n",
    "        for splitter in split_list:\n",
    "            temp_list = []\n",
    "            for sub_element in split_elements:\n",
    "                temp_list.extend(sub_element.split(splitter))\n",
    "            split_elements = temp_list\n",
    "        \n",
    "        # Remove empty strings resulting from split\n",
    "        split_elements = [elem for elem in split_elements if elem]\n",
    "\n",
    "        new_list.extend(split_elements)\n",
    "        new_index.extend([index] * len(split_elements))\n",
    "\n",
    "    return new_list, new_index\n",
    "\n",
    "# Example usage\n",
    "Test_list = [\"Sentence. New sentence and a bit more\", \"Something else\", \"More stuff and even more\"]\n",
    "Test_index = [0, 1, 2]\n",
    "SplitList = [\".\", \" and\"]\n",
    "\n",
    "New_list, New_index = split_and_update_indices(Test_list, Test_index, SplitList)\n",
    "\n",
    "print(New_list)\n",
    "print(New_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(DataSet=\"Heat\", SplitterList=[\". \", \", \", \"-\", \"and\", \";\"], Type=\"Split\", index=\"ID\"):\n",
    "    \"\"\"\n",
    "    Load in the dataset, process it into a list of data.\n",
    "        \n",
    "    Parameters:\n",
    "    DataSet (str): The dataset to load in. Options are \"Heat\", \"Cold\" or \"Flood\".\n",
    "    SplitterList (list): List of strings to split the documents by, only applies if Type is \"Split\".\n",
    "    Type (str): The type of processing to apply to the dataset. Options are \"Split\" or \"Combine\".\n",
    "    index (str): The column to use as an index for the data. Useful if you want to match documents in your topic model to the original data.\n",
    "    If None, then it just uses the index from the loaded dataframe.\n",
    "\n",
    "    If Type = \"Combine\" then any info from all 3 potential columns are combined into 1 entry per person.\n",
    "    This appears ideal but the larger the sentence, the harder it is for BERT to deduce what sentences are similar.\n",
    "\n",
    "    If Type = \"Split\" then the data is split by the SplitterList and each entry is split into multiple entries.\n",
    "    A separate index is outputted which corresponds to the original data for each entry in the list \n",
    "    (this even works for the splitter option).\n",
    "\n",
    "    Returns:\n",
    "    docs (list): List of strings to use for topic modelling.\n",
    "    doc_index (list): List of indices corresponding to the strings in docs.\n",
    "    zero_shot_topic_list_major (list): List of major topics for zero-shot classification.\n",
    "    zero_shot_topic_list_minor (list): List of minor topics for zero-shot classification.\n",
    "    \"\"\"\n",
    "    \n",
    "    if DataSet == \"Heat\":\n",
    "        # Heat Data\n",
    "        RawText = pd.read_excel(\"HeatSurvey23 Analysis_Free TextHuwsCopy.xlsx\", sheet_name=\"Huws Breakdown (2)\")\n",
    "        RawTextAnalysis = RawText[[\"ADVNC_TEXT\", \"WTHR_TEXT\", \"FREE_TEXT\"]]\n",
    "\n",
    "        zero_shot_topic_list_major = [\"Routine\", \"Technology solution\", \"Home adaptations\", \"Personal care\"]\n",
    "        zero_shot_topic_list_minor = [\"Alternative plans\", \"Time of day\", \"Reduced activity\", \n",
    "                                          \"Benefit\", \"Physical health\", \"Cooling\", \"Sun protection\", \n",
    "                                          \"Care\", \"Fan\", \"Air conditioning\", \"Closed curtains or blinds\", \n",
    "                                          \"Lighter bedding\"]\n",
    "    elif DataSet == \"Cold\":\n",
    "        # Cold Data\n",
    "        RawText = pd.read_excel(\"ColdFloodSurvey24 AnalysisHuwsCopy.xlsx\", sheet_name=\"HuwsDATA\", header=1)\n",
    "        RawTextAnalysis = RawText[[\"ADVNC_TEXT_C\", \"WTHR_TEXT_C\", \"FREE_TEXT_C\"]] \n",
    "\n",
    "        zero_shot_topic_list_major = [\"Routine\", \"Technology solution\", \"Home adaptations\", \"Personal care\", \"Safety precautions\"]\n",
    "        zero_shot_topic_list_minor = [\"Time of day\", \"Alternative plans\", \"Reduced activity\", \"Benefit\", \"Physical health\", \n",
    "                                          \"Extra Heating\", \"Dehumidifier\", \"Blankets\", \"Insulation\", \"Layers\", \"Hot water bottle\", \n",
    "                                          \"Hot drinks\", \"Bigger meals\", \"Car\", \"Pipes\", \"Supplies\", \"Family\"]\n",
    "    elif DataSet == \"Flood\":    \n",
    "        # Storm Data\n",
    "        RawText = pd.read_excel(\"ColdFloodSurvey24 AnalysisHuwsCopy.xlsx\", sheet_name=\"HuwsDATA\", header=1)\n",
    "        RawTextAnalysis = RawText[[\"ADVNC_TEXT_F\", \"WTHR_TEXT_F\", \"FREE_TEXT_F\"]]\n",
    "\n",
    "        zero_shot_topic_list_major = [\"Routine\", \"Technology solution\", \"Personal care\", \"Safety precautions\"]\n",
    "        zero_shot_topic_list_minor = [\"Time of day\", \"Alternative plans\", \"Reduced activity\", \"Heating\", \"Clothing\", \"Secure loose objects\", \"Drainage\"]\n",
    "    else:\n",
    "        print(\"Invalid DataSet\")\n",
    "        exit()\n",
    "    \n",
    "    print(\"Data loaded: \", DataSet)\n",
    "    print(\"Number of rows: \", RawTextAnalysis.shape[0])\n",
    "    # Find how many rows have only NaNs or missing values\n",
    "    missing_values_count = RawTextAnalysis.isnull().all(axis=1).sum()\n",
    "    print(\"Number of rows with no responses: \", missing_values_count)\n",
    "    print(\"Percentage of rows with no responses: \", missing_values_count / RawTextAnalysis.shape[0] * 100)\n",
    "\n",
    "    #HeatTextAnalysis = HeatText[[\"ADVNC_TEXT\", \"WTHR_TEXT\", \"OTHR_TEXT\", \"FREE_TEXT\"]]\n",
    "    #.drop([\"ID\", \"WTHR\", \"OTHR\", \"ALERT\", \"ADVNC\", \"HEALTH\", \"Health Conditions\", \"Symtoms1\", \"Symtoms2\", \"Symtoms3\"], axis=1)\n",
    "    if Type == \"Split\":\n",
    "        data = RawTextAnalysis.values.flatten().tolist()\n",
    "        dataset_size = len(data)\n",
    "        # Remove NaN values from the list\n",
    "        docs = [value for value in data if not (isinstance(value, float) and math.isnan(value))]\n",
    "        removed_size = len(docs)\n",
    "\n",
    "        print(\"List length: \", dataset_size)\n",
    "        print(\"Removed NaN values from list: \", dataset_size - removed_size)\n",
    "        print(\"Percentage non-responses removed: \", (dataset_size - removed_size) / dataset_size * 100)\n",
    "        print(\"Final dataset size: \", removed_size)\n",
    "\n",
    "        if index == False:\n",
    "            doc_index = []\n",
    "        else:\n",
    "            doc_index = RawText[index].tolist()\n",
    "\n",
    "        for element in docs:\n",
    "            doc_index.append(data.index(element))\n",
    "        \n",
    "        if len(SplitterList) > 0:\n",
    "            print(\"Length of docs before splitting: \", len(docs))\n",
    "            print(\"Items to split by: \", SplitterList)\n",
    "            docs, doc_index = split_and_update_indices(docs, doc_index, SplitterList)\n",
    "            print(\"Length of docs after splitting: \", len(docs))\n",
    "\n",
    "    elif Type == \"Combine\":\n",
    "        temp_index = RawText[index]\n",
    "        RawTextAnalysis = pd.merge(temp_index, RawTextAnalysis, left_index=True, right_index=True)\n",
    "\n",
    "        dataset_size = RawTextAnalysis.shape[0]\n",
    "        dataset = RawTextAnalysis.dropna(how=\"all\", subset=RawTextAnalysis.columns[1:])\n",
    "\n",
    "        if index == False:\n",
    "            doc_index = dataset[index].tolist()\n",
    "        else:\n",
    "            doc_index = RawText[index]\n",
    "            \n",
    "        removed_size = dataset.shape[0]\n",
    "\n",
    "        print(\"List length: \", dataset_size)\n",
    "        print(\"Removed NaN values from list: \", dataset_size - removed_size)\n",
    "        print(\"Percentage non-responses removed: \", (dataset_size - removed_size) / dataset_size * 100)\n",
    "        print(\"Final dataset size: \", removed_size)\n",
    "\n",
    "        docs = []\n",
    "\n",
    "        for row in dataset.iterrows():\n",
    "            values = row[1].dropna().values\n",
    "            #print(values)\n",
    "            value_combined = \". \".join([str(value) for value in values if value is not None])\n",
    "            #print(value_combined)\n",
    "            #print(\"\")\n",
    "            docs.append(value_combined)\n",
    "\n",
    "        doclen = len(docs)    \n",
    "        docs = [doc for doc in docs if re.search('[a-zA-Z]', doc)]\n",
    "        if doclen != len(docs):\n",
    "            print(\"Removed empty strings from dataset\")\n",
    "            print(\"Length of docs after processing: \", len(docs))\n",
    "            \n",
    "    else:\n",
    "        print(\"Invalid Process Type\")\n",
    "        exit()\n",
    "                     \n",
    "    return docs, doc_index, zero_shot_topic_list_major, zero_shot_topic_list_minor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outputting number of rows with no responses from uncleaned data\n",
    "\n",
    "RawText = pd.read_excel(\"HeatSurvey23 Analysis_Free TextHuwsCopy.xlsx\", sheet_name=\"FreeText Breakdown\")\n",
    "RawTextAnalysis = RawText[[\"ADVNC_TEXT\", \"WTHR_TEXT\", \"FREE_TEXT\"]]\n",
    "\n",
    "print(\"Data loaded: Raw Heat\")\n",
    "print(\"Number of rows: \", RawTextAnalysis.shape[0])\n",
    "# Find how many rows have only NaNs or missing values\n",
    "missing_values_count = RawTextAnalysis.isnull().all(axis=1).sum()\n",
    "print(\"Number of rows with no responses: \", missing_values_count)\n",
    "print(\"Percentage of rows with no responses: \", missing_values_count / RawTextAnalysis.shape[0] * 100)\n",
    "print(\"\")\n",
    "\n",
    "RawText = pd.read_excel(\"ColdFloodSurvey24 AnalysisHuwsCopy.xlsx\", sheet_name=\"DATA\", header=1)\n",
    "RawTextAnalysis = RawText[[\"ADVNC_TEXT_C\", \"WTHR_TEXT_C\", \"FREE_TEXT_C\"]] \n",
    "\n",
    "print(\"Data loaded: Raw Cold\")\n",
    "print(\"Number of rows: \", RawTextAnalysis.shape[0])\n",
    "# Find how many rows have only NaNs or missing values\n",
    "missing_values_count = RawTextAnalysis.isnull().all(axis=1).sum()\n",
    "print(\"Number of rows with no responses: \", missing_values_count)\n",
    "print(\"Percentage of rows with no responses: \", missing_values_count / RawTextAnalysis.shape[0] * 100)\n",
    "print(\"\")\n",
    "\n",
    "RawTextAnalysis = RawText[[\"ADVNC_TEXT_F\", \"WTHR_TEXT_F\", \"FREE_TEXT_F\"]]\n",
    "\n",
    "print(\"Data loaded: Raw Flood\")\n",
    "print(\"Number of rows: \", RawTextAnalysis.shape[0])\n",
    "# Find how many rows have only NaNs or missing values\n",
    "missing_values_count = RawTextAnalysis.isnull().all(axis=1).sum()\n",
    "print(\"Number of rows with no responses: \", missing_values_count)\n",
    "print(\"Percentage of rows with no responses: \", missing_values_count / RawTextAnalysis.shape[0] * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Heat, Heat_index, major_topics_heat, minor_topics_heat = data_loader(\"Heat\")\n",
    "print(\"\")\n",
    "print(\"Has the index updater worked: \", len(Heat) == len(Heat_index))\n",
    "print(\"\")\n",
    "HeatFrame = pd.DataFrame({\"Document\": Heat, \"Index\": Heat_index})\n",
    "HeatFrame.to_csv(\"DocumentProcessing/HeatData.csv\", index=False)\n",
    "\n",
    "Cold, Cold_index, major_topics_cold, minor_topics_cold = data_loader(\"Cold\")\n",
    "print(\"\")\n",
    "print(\"Has the index updater worked: \", len(Cold)==len(Cold_index))\n",
    "print(\"\")\n",
    "ColdFrame = pd.DataFrame({\"Document\": Cold, \"Index\": Cold_index})\n",
    "ColdFrame.to_csv(\"DocumentProcessing/ColdData.csv\", index=False)\n",
    "\n",
    "Flood, Flood_index, major_topics_flood, minor_topics_flood = data_loader(\"Flood\")\n",
    "print(\"\")\n",
    "print(\"Has the index updater worked: \", len(Flood)==len(Flood_index))\n",
    "print(\"\")\n",
    "FloodFrame = pd.DataFrame({\"Document\": Flood, \"Index\": Flood_index})\n",
    "FloodFrame.to_csv(\"DocumentProcessing/FloodData.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Name = \"Cold\" # Set this to be one of the following: \"Heat\", \"Cold\", \"Flood\"\n",
    "\n",
    "docs, docs_index, zero_shot_topic_list_major, zero_shot_topic_list_minor = data_loader(Name)\n",
    "print(\"\")\n",
    "\n",
    "# print(\"Example docs: \")\n",
    "# for doc in docs[:15]:\n",
    "#     print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can remove stop works that include stuff like \"see previous question\" or \"no answer\" this only removes them from the representation of the data, not the actual data.\n",
    "If you remove all the viable representation then you'll still get topic clusters but they'll just have no representation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "free_s_w = [\"made\", \"sure\"] \n",
    "stop_words.extend(free_s_w)\n",
    "\n",
    "# Print stop words across several lines to make it easier to read\n",
    "for i in range(0, 5):\n",
    "    print(stop_words[i:i+10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "\n",
    "\n",
    "# Step 1 - Extract embeddings\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = embedding_model.encode(docs, show_progress_bar=True)\n",
    "\n",
    "# Step 1 is by far the longest part of the process, so I recommend running this as few times as possible, especially for larger datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 - Reduce dimensionality\n",
    "umap_model = UMAP(n_neighbors=20, n_components=5, min_dist=0.0, metric='cosine', random_state=42)\n",
    "# High number of nearest neighbors and low minimum distance can help to preserve local structure\n",
    "\n",
    "# Step 3 - Cluster reduced embeddings\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=60, min_samples=1, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
    "# The parameter \"min_cluster_size\" here lets you indirectly influence the number of clusters by setting a minimum size for each cluster.\n",
    "# Setting \"min_samples\" to a lower value can help you reduce the size of the outliers cluster.\n",
    "\n",
    "# Step 4 - Tokenize topics\n",
    "vectorizer_model = CountVectorizer(stop_words=stop_words, min_df=1, ngram_range=(1,3))\n",
    "\n",
    "# Step 5 - Create topic representation\n",
    "ctfidf_model = ClassTfidfTransformer()\n",
    "\n",
    "# Step 6 - (Optional) Fine-tune topic representations\n",
    "representation_model = MaximalMarginalRelevance(diversity=0.7)\n",
    "# 0 being not diverse, 1 being most diverse\n",
    "\n",
    "# All steps together\n",
    "BERT_model = BERTopic(\n",
    "    embedding_model=embedding_model,          # Step 1 - Extract embeddings\n",
    "    umap_model=umap_model,                    # Step 2 - Reduce dimensionality\n",
    "    hdbscan_model=hdbscan_model,              # Step 3 - Cluster reduced embeddings\n",
    "    vectorizer_model=vectorizer_model,        # Step 4 - Tokenize topics\n",
    "    ctfidf_model=ctfidf_model,                # Step 5 - Extract topic words\n",
    "    representation_model=representation_model, # Step 6 - (Optional) Fine-tune topic representations\n",
    "\n",
    "    #zeroshot_topic_list=zero_shot_topic_list_minor, zeroshot_min_similarity=0.9, # Comment this out if you don't want zero-shot\n",
    "    # 0 similarity = only use my topic lists, 1 similarity = barely pay attention to my topic lists.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics, _ = BERT_model.fit_transform(docs)\n",
    "\n",
    "# You can also reduce outliers after BERTopic has been fitted\n",
    "new_topics = BERT_model.reduce_outliers(docs, topics)\n",
    "\n",
    "# # Reduce outliers with pre-calculate embeddings instead\n",
    "# new_topics = BERT_model.reduce_outliers(docs, topics, strategy=\"embeddings\", embeddings=embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_inf = BERT_model.get_document_info(docs)\n",
    "doc_inf[\"Person_id\"] = docs_index\n",
    "doc_inf.to_csv(f\"BERTModelRawOutputs/{Name}_Document_Info.csv\")\n",
    "doc_inf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_inf = BERT_model.get_topic_info()\n",
    "# Calculate the number of unique person IDs per topic\n",
    "unique_person_ids_per_topic = doc_inf.groupby('Topic')['Person_id'].nunique().reset_index()\n",
    "unique_person_ids_per_topic.columns = ['Topic', 'Unique_Person_Count']\n",
    "\n",
    "# Merge with the topic information dataframe\n",
    "top_inf = top_inf.merge(unique_person_ids_per_topic, on='Topic', how='left')\n",
    "top_inf.to_csv(f\"BERTModelRawOutputs/{Name}_Topic_Info.csv\")\n",
    "top_inf.head()\n",
    "\n",
    "# Topic -1 is the \"outlier\" topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = doc_inf[\"Name\"].value_counts(ascending=True).plot(kind='barh', xlabel=\"Number of responses\", ylabel=\"Topic\", title=\"Number of responses per topic\")\n",
    "fig3 = ax.get_figure()\n",
    "fig3.savefig(f\"BERTModelRawOutputs/{Name}_Number_of_responses_per_topic.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# Create the bar plot\n",
    "ax = sns.barplot(x=\"Unique_Person_Count\", y=\"Name\", data=top_inf)\n",
    "\n",
    "# Set the labels and title\n",
    "ax.set(xlabel=\"Number of unique people\", ylabel=\"Topic\", title=\"Number of unique people per topic\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "fig = ax.get_figure()\n",
    "fig.savefig(f\"BERTModelRawOutputs/{Name}_Number_of_unique_people_per_topic.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_topics, similarity = BERT_model.find_topics(\"elderly\", top_n=5)\n",
    "BERT_model.get_topic(similar_topics[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_topics = len(BERT_model.get_topic_info())\n",
    "fig = BERT_model.visualize_barchart(top_n_topics=number_of_topics)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2 = BERT_model.visualize_hierarchy()\n",
    "fig2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topics_to_merge = [[1, 2]\n",
    "#                    [3, 4]]\n",
    "# BERT_model.merge_topics(docs, topics_to_merge)\n",
    "\n",
    "\n",
    "\n",
    "#BERT_model.reduce_topics(docs, nr_topics=30)\n",
    "\n",
    "\n",
    "# Topics_to_merge = []\n",
    "# merge_list = []\n",
    "# while True:\n",
    "#     print(\"Current merge list: \", merge_list)\n",
    "#     print(\"Current Topics_to_merge: \", Topics_to_merge)\n",
    "#     topic = input(\"Enter topic numbers to merge, 'm' to conclude that merge list and 'q' to quit: \")\n",
    "#     if topic == \"q\":\n",
    "#         break\n",
    "#     elif topic == \"m\":\n",
    "#         if len(merge_list) > 1:\n",
    "#             Topics_to_merge.append(merge_list)\n",
    "#         merge_list = []\n",
    "#     else:\n",
    "#         merge_list.append(int(topic))\n",
    "# if len(Topics_to_merge) > 0:\n",
    "#     print(\"Topics_to_merge: \", Topics_to_merge)\n",
    "#     BERT_model.merge_topics(docs=docs, topics_to_merge=Topics_to_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotly.graph_objects as go\n",
    "\n",
    "# # Assuming `BERT_model` is already defined\n",
    "# topic_info = BERT_model.get_topic_info()\n",
    "# num_topics = len(topic_info) - 1  # Exclude the outlier topic (-1)\n",
    "\n",
    "# topn_words = 4  # Number of top words to display for each topic\n",
    "\n",
    "# labels = []\n",
    "# parents = []\n",
    "# values = []\n",
    "\n",
    "# # Add topics to the sunburst plot\n",
    "# for i in range(num_topics):\n",
    "#     topic = BERT_model.get_topic(i)\n",
    "#     topic_label = f\"Topic {i+1}\"\n",
    "#     labels.append(topic_label)\n",
    "#     parents.append(\"\")\n",
    "#     values.append(sum([weight for _, weight in topic[:topn_words]]))\n",
    "\n",
    "#     # Add top words for each topic\n",
    "#     for word, weight in topic[:topn_words]:\n",
    "#         labels.append(word)\n",
    "#         parents.append(topic_label)\n",
    "#         values.append(weight)\n",
    "\n",
    "# # Print lists for debugging\n",
    "# print(labels)\n",
    "# print(parents)\n",
    "# print(values)\n",
    "\n",
    "# # Create the sunburst plot\n",
    "# fig = go.Figure(go.Sunburst(\n",
    "#     labels=labels,\n",
    "#     parents=parents,\n",
    "#     values=values,\n",
    "#     branchvalues=\"total\",\n",
    "#     hovertemplate='<b>%{label}</b><br>Weight: %{value:.2f}<extra></extra>',\n",
    "# ))\n",
    "\n",
    "# # Set the layout\n",
    "# fig.update_layout(\n",
    "#     title=\"BERTopic Model Sunburst Plot for Heat Survey\",\n",
    "#     margin=dict(t=50, l=0, r=0, b=0),\n",
    "# )\n",
    "\n",
    "# # Show the plot\n",
    "# fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bertopic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
